{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import kurtosis, skew\n",
    "import time\n",
    "import scipy\n",
    "import sklearn\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "def calculate_landmark_lda(X, y):\n",
    "    import sklearn.discriminant_analysis\n",
    "    if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "        kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "    else:\n",
    "        kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "\n",
    "    accuracy = 0.\n",
    "    result=kf.split(X, y) \n",
    "    \n",
    "    try:\n",
    "        for train,test in kf.split(X,y):   \n",
    "   \n",
    "            lda = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "\n",
    "            if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "                lda.fit(X.iloc[train], y[train])\n",
    "            else:\n",
    "                lda = OneVsRestClassifier(lda)\n",
    "                lda.fit(X.iloc[train], y[train])\n",
    "\n",
    "            predictions = lda.predict(X.iloc[test])\n",
    "            accuracy += sklearn.metrics.accuracy_score(predictions, y[test])\n",
    "        return accuracy / 10\n",
    "    except scipy.linalg.LinAlgError as e:\n",
    "        print(\"LDA failed: %s Returned 0 instead!\" % e)\n",
    "        return np.NaN\n",
    "    except ValueError as e:\n",
    "        print(\"LDA failed: %s Returned 0 instead!\" % e)\n",
    "        return np.NaN\n",
    "\n",
    "\n",
    "\n",
    "def calculate_landmark_nb(X, y):\n",
    "    import sklearn.naive_bayes\n",
    "\n",
    "    if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "        kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "    else:\n",
    "        kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "\n",
    "    accuracy = 0.\n",
    "    for train, test in kf.split(X, y):\n",
    "        nb = sklearn.naive_bayes.GaussianNB()\n",
    "\n",
    "        if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "            nb.fit(X.iloc[train], y[train])\n",
    "        else:\n",
    "            nb = OneVsRestClassifier(nb)\n",
    "            nb.fit(X.iloc[train], y[train])\n",
    "\n",
    "        predictions = nb.predict(X.iloc[test])\n",
    "        accuracy += sklearn.metrics.accuracy_score(predictions, y[test])\n",
    "    return accuracy / 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_landmark_dt(X, y):\n",
    "    import sklearn.tree\n",
    "\n",
    "    if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "        kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "    else:\n",
    "        kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "\n",
    "    accuracy = 0.\n",
    "    for train, test in kf.split(X, y):\n",
    "        random_state = sklearn.utils.check_random_state(42)\n",
    "        tree = sklearn.tree.DecisionTreeClassifier(random_state=random_state)\n",
    "\n",
    "        if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "            tree.fit(X.iloc[train], y[train])\n",
    "        else:\n",
    "            tree = OneVsRestClassifier(tree)\n",
    "            tree.fit(X.iloc[train], y[train])\n",
    "\n",
    "        predictions = tree.predict(X.iloc[test])\n",
    "        accuracy += sklearn.metrics.accuracy_score(predictions, y[test])\n",
    "    return accuracy / 10\n",
    "\n",
    "\n",
    "def calculate_landmark_dnl(X, y):\n",
    "        import sklearn.tree\n",
    "\n",
    "        if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "            kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "        else:\n",
    "            kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "\n",
    "        accuracy = 0.\n",
    "        for train, test in kf.split(X, y):\n",
    "            random_state = sklearn.utils.check_random_state(42)\n",
    "            node = sklearn.tree.DecisionTreeClassifier(\n",
    "                criterion=\"entropy\", max_depth=1, random_state=random_state,\n",
    "                min_samples_split=2, min_samples_leaf=1,  max_features=None)\n",
    "            if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "                node.fit(X.iloc[train], y[train])\n",
    "            else:\n",
    "                node = OneVsRestClassifier(node)\n",
    "                node.fit(X.iloc[train], y[train])\n",
    "            predictions = node.predict(X.iloc[test])\n",
    "            accuracy += sklearn.metrics.accuracy_score(predictions, y[test])\n",
    "        return accuracy / 10\n",
    "\n",
    "    \n",
    "\n",
    "def calculate_landmark_rnl(X, y):\n",
    "    import sklearn.tree\n",
    "\n",
    "    if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "        kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "    else:\n",
    "        kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "    accuracy = 0.\n",
    "\n",
    "    for train, test in kf.split(X, y):\n",
    "        random_state = sklearn.utils.check_random_state(42)\n",
    "        node = sklearn.tree.DecisionTreeClassifier(criterion=\"entropy\",max_depth=1,\\\n",
    "        random_state=random_state, min_samples_split=2, min_samples_leaf=1, max_features=1)\n",
    "        node.fit(X.iloc[train], y[train])\n",
    "        predictions = node.predict(X.iloc[test])\n",
    "        accuracy += sklearn.metrics.accuracy_score(predictions, y[test])\n",
    "    return accuracy / 10\n",
    "\n",
    "\n",
    "\n",
    "def calculate_landmark_k1nn(X, y):\n",
    "    import sklearn.neighbors\n",
    "\n",
    "    if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "        kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "    else:\n",
    "        kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "\n",
    "    accuracy = 0.\n",
    "    for train, test in kf.split(X, y):\n",
    "        kNN = sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "        if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "            kNN.fit(X.iloc[train], y[train])\n",
    "        else:\n",
    "            kNN = OneVsRestClassifier(kNN)\n",
    "            kNN.fit(X.iloc[train], y[train])\n",
    "        predictions = kNN.predict(X.iloc[test])\n",
    "        accuracy += sklearn.metrics.accuracy_score(predictions,y[test])\n",
    "    return accuracy / 10\n",
    "\n",
    "\n",
    "\n",
    "### Not a meta feature just an object\n",
    "def calculate_pca(X, y):\n",
    "    import sklearn.decomposition\n",
    "    pca = sklearn.decomposition.PCA(copy=True)\n",
    "    rs = np.random.RandomState(42)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            rs.shuffle(indices)\n",
    "            pca.fit(X.iloc[indices])\n",
    "            return pca\n",
    "        except LinAlgError as e:\n",
    "            pass\n",
    "    print(\"Failed to compute a Principle Component Analysis\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def calculate_PCAFractionOfComponentsFor95PercentVariance(X, y, pca):\n",
    "\n",
    "    if pca is None:\n",
    "        return np.NaN\n",
    "    sum_ = 0.\n",
    "    idx = 0\n",
    "    while sum_ < 0.95 and idx < len(pca.explained_variance_ratio_):\n",
    "        sum_ += pca.explained_variance_ratio_[idx]\n",
    "        idx += 1\n",
    "    return float(idx)/float(X.shape[1])\n",
    "\n",
    "\n",
    "def calculate_PCAKurtosisFirstPC(X, y, pca):\n",
    "    if pca is None:\n",
    "        return np.NaN\n",
    "    components = pca.components_\n",
    "    pca.components_ = components[:1]\n",
    "    transformed = pca.transform(X)\n",
    "    pca.components_ = components\n",
    "\n",
    "    kurtosis = scipy.stats.kurtosis(transformed)\n",
    "    return kurtosis[0]\n",
    "\n",
    "def calculate_PCASkewnessFirstPC(X, y, pca):\n",
    "    if pca is None:\n",
    "        return np.NaN\n",
    "    components = pca.components_\n",
    "    pca.components_ = components[:1]\n",
    "    transformed = pca.transform(X)\n",
    "    pca.components_ = components\n",
    "\n",
    "    skewness = scipy.stats.skew(transformed)\n",
    "    return skewness[0]\n",
    "\n",
    "\n",
    "meta_features_names=[\"nr_instances\", \"log_nr_instances\", \"nr_features\",\\\n",
    "    \"log_nr_features\", \"nr_classes\", \"nr_numerical_features\", \"nr_categorical_features\", \\\n",
    "    \"ratio_num_cat\", \"class_entropy\", \"missing_val\", \"ratio_missing_val\", \"max_prob\", \\\n",
    "    \"min_prob\", \"mean_prob\",\"std_dev\", \"dataset_ratio\", \"symbols_sum\", \"symbols_mean\", \\\n",
    "    \"symbols_std_dev\", \"skew_min\", \"skew_max\", \"skew_mean\", \"skew_std_dev\", \"kurtosis_min\",\\\n",
    "    \"kurtosis_max\", \"kurtosis_mean\", \"kurtosis_std_dev\"]\n",
    "                     #\"landmark_lda\", \"landmark_nb\",\"landmark_dt\",\\\n",
    "    #\"landmark_dnl\",\"landmark_rnl\", \"landmark_k1nn\", \"landmark_PCAFractionOfComponentsFor95PercentVariance\",\\\n",
    "    #\"landmark_PCAKurtosisFirstPC\", \"landmark_PCASkewnessFirstPC\"]\n",
    "\n",
    "def  extractMetaFeatures(dataset, file, classCol = None):\n",
    "    \n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    #### 5. Number of Classes - DONE\n",
    "    if classCol == None:\n",
    "        target_variable_Index = dataset.shape[1] - 1\n",
    "    else:\n",
    "         target_variable_Index = dataset.columns.get_loc(classCol)\n",
    "        \n",
    "    target_variable = dataset.iloc[:, target_variable_Index]\n",
    "    dataset.drop(dataset.columns[target_variable_Index], axis=1, inplace = True)\n",
    "    \n",
    "    nr_classes = target_variable.nunique()\n",
    "    print(nr_classes)\n",
    "    t1 = time.time()\n",
    "    #print('1. Time spend:', t1 - t0)\n",
    "    #### Remove Missing Values\n",
    "    dataset.replace(to_replace = [\"? \",\"?\", \" ?\" \"-\" \" -\",\"- \", \" - \", \"#\", \" #\", \"# \",\" # \", \" \"], value = np.NAN, inplace = True)\n",
    "    dataset.dropna(axis = 1, how = 'all', inplace = True) # Drop Column if all of its values are missing\n",
    "\n",
    "    #### Remove ID columns or columns with always the same value\n",
    "    for col in dataset.columns:\n",
    "        feature = dataset[col].dropna()\n",
    "        numSyms = feature.nunique()\n",
    "        if col == 'id' or col == 'ID' or numSyms == 1 or (numSyms == dataset.shape[0] and feature.dtype != np.number):\n",
    "            dataset.drop(col, axis = 1, inplace = True)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    #print('Preprocessing Time spent:', t0 - t1)\n",
    "    \n",
    "    ### 1.Number of Instances - DONE\n",
    "    nr_instances = dataset.shape[0]\n",
    "    t1 = time.time()\n",
    "    #print('1. Time spend:', t1 - t0)\n",
    "    \n",
    "    ### 2.Log number of Instances - DONE\n",
    "    log_nr_instances = np.log(nr_instances)\n",
    "    t2 = time.time()\n",
    "    #print('2. Time spend:', t2 - t1)\n",
    "    \n",
    "    #### 3.Number of Features - DONE\n",
    "    nr_features = dataset.shape[1]\n",
    "    t3 = time.time()\n",
    "    #print('3. Time spend:', t3 - t2)\n",
    "    \n",
    "    ### 4.Log Number of Features - DONE\n",
    "    log_nr_features = np.log(nr_features)\n",
    "    t4 = time.time()\n",
    "    #print('4. Time spend:', t4 - t3)\n",
    "    \n",
    "\n",
    "    ### 6. Number of Missing Values\n",
    "    ### 7. Ratio of Missing Values - DONE\n",
    "    missing_val = 0\n",
    "    missing_val = dataset.isnull().sum().sum() + dataset.isna().sum().sum()\n",
    "    \n",
    "    ratio_missing_val = missing_val / dataset.size\n",
    "    t7 = time.time()\n",
    "    #print('7. Time spend:', t7 - t6)\n",
    "    \n",
    "    ### 8. Number of Numerical Features - DONE\n",
    "    ### 9. Number of Categorical Features - DONE\n",
    "    numerical = []\n",
    "    categorical = dataset.select_dtypes(exclude=['number']).columns.values.tolist()\n",
    "    for col in dataset.columns:\n",
    "        if col not in categorical:\n",
    "            feature = dataset[col].dropna()\n",
    "            numSyms = feature.nunique()\n",
    "            if numSyms < log_nr_instances: #it should be considered as categorical if number of unique values < log number of instances\n",
    "                categorical.append(col)\n",
    "            else:\n",
    "                numerical.append(col)\n",
    "    \n",
    "    nr_numerical_features = len(numerical)\n",
    "    nr_categorical_features = len(categorical)\n",
    "    \n",
    "    t9 = time.time()\n",
    "    #print('9. Time spend:', t9 - t7)\n",
    "                \n",
    "    ### 10. Ratio of Categorical to Numerical Features - DONE\n",
    "    if(nr_numerical_features > 0):\n",
    "        ratio_num_cat = nr_categorical_features / nr_numerical_features\n",
    "    else:\n",
    "        ratio_num_cat = 9999999999\n",
    "    t10 = time.time()\n",
    "    #print('10. Time spend:', t10 - t9)\n",
    "    \n",
    "    ### 11. Class Entropy - DONE\n",
    "    prob_classes = []\n",
    "    class_entropy = 0\n",
    "    classes = target_variable.unique()\n",
    "    \n",
    "    for value in classes:\n",
    "        prob = (sum(target_variable==value) / len(target_variable))\n",
    "        \n",
    "        prob_classes.append(prob)\n",
    "        class_entropy = class_entropy - prob * np.log2(prob)\n",
    "        \n",
    "    ### 12. Maximum Class probability - DONE\n",
    "    max_prob = max(prob_classes)\n",
    "    \n",
    "    ### 13. Minimum Class probability - DONE\n",
    "    min_prob = min(prob_classes)\n",
    "    \n",
    "    ### 14. Mean Class probability - DONE\n",
    "    mean_prob = np.mean(prob_classes)\n",
    "    \n",
    "    ### 15. Standard Deviation of Class probability - DONE\n",
    "    std_dev = np.std(prob_classes)\n",
    "    \n",
    "    ### 16. Dataset Ratio - DONE\n",
    "    dataset_ratio = nr_features / nr_instances\n",
    "    \n",
    "    t11 = time.time()\n",
    "    #print('11. Time spend:', t11 - t10)\n",
    "    \n",
    "    ### Categorical Features Statistics\n",
    "    symbols=[]\n",
    "    if len(categorical) != 0:\n",
    "        for col in categorical:\n",
    "            feature = dataset[col].dropna()\n",
    "            symbols.append(feature.nunique())\n",
    "    \n",
    "    ### 17. Symbols Sum - DONE\n",
    "        symbols_sum = sum(symbols)\n",
    "        \n",
    "    ### 18. Symbols Mean - DONE\n",
    "        symbols_mean = np.mean(symbols)\n",
    "\n",
    "    ### 19. Symbols Standard Deviation - DONE\n",
    "        symbols_std_dev = np.std(symbols)\n",
    "        \n",
    "    else:\n",
    "        symbols_sum = 0\n",
    "        symbols_mean = 0\n",
    "        symbols_std_dev = 0\n",
    "    \n",
    "    t12 = time.time()\n",
    "    #print('12. Time spend:', t12 - t11)\n",
    "    \n",
    "    ### Numerical Features Statistics\n",
    "    skewness_values = np.zeros(len(numerical))\n",
    "    kurtosis_values = np.zeros(len(numerical))\n",
    "    \n",
    "    #print(dataset)\n",
    "    if len(numerical) != 0:\n",
    "        for coli in range(len(numerical)):\n",
    "            #print(numerical[coli])\n",
    "            feature = dataset[numerical[coli]].dropna()\n",
    "            #print('feat-AFTER:', feature)\n",
    "            skewness = skew(feature)\n",
    "            kurt = kurtosis(feature)\n",
    "            skewness_values[coli] = skewness\n",
    "            kurtosis_values[coli] = kurt\n",
    "    \n",
    "    ### 20. Skewness Minimum - DONE\n",
    "        skew_min = min(skewness_values)\n",
    "\n",
    "    ### 21. Skewness Maximum - DONE\n",
    "        skew_max = max(skewness_values)\n",
    "\n",
    "    ### 22. Skewness Mean - DONE\n",
    "        skew_mean = np.mean(skewness_values)\n",
    "\n",
    "    ### 23. Skewness Standard deviation - DONE\n",
    "        skew_std_dev = np.std(skewness_values)\n",
    "\n",
    "    ### 24. Kurtosis Minimum - DONE\n",
    "        kurtosis_min = min(kurtosis_values)\n",
    "\n",
    "    ### 25. Kurtosis Maximum - DONE\n",
    "        kurtosis_max = max(kurtosis_values)\n",
    "\n",
    "    ### 26. Kurtosis Mean - DONE\n",
    "        kurtosis_mean = np.mean(kurtosis_values)\n",
    "\n",
    "    ### 27. Kurtosis Standard Deviation - DONE\n",
    "        kurtosis_std_dev = np.std(kurtosis_values)\n",
    "    else:\n",
    "        skew_min = 0\n",
    "        skew_max = 0\n",
    "        skew_mean = 0\n",
    "        skew_std_dev = 0\n",
    "        kurtosis_min = 0\n",
    "        kurtosis_max = 0\n",
    "        kurtosis_mean = 0\n",
    "        kurtosis_std_dev = 0\n",
    "    \n",
    "    #t13 = time.time()\n",
    "    #print('13. Time spend:', t13 - t12)    \n",
    "    \n",
    "    #class_col=dataset.iloc[:,-1]\n",
    "    #dataset=pd.get_dummies(dataset.iloc[:,:-1])\n",
    "    #dataset[\"class\"]=class_col\n",
    "    \n",
    "        \n",
    "    y=target_variable\n",
    "    #print(y.nunique())\n",
    "    \n",
    "    X = dataset\n",
    "    X = pd.get_dummies(X)\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y=le.fit_transform(y.astype(str))\n",
    "    #landmark_lda = calculate_landmark_lda(X,y)\n",
    "   # landmark_nb = calculate_landmark_nb(X,y)\n",
    "    #landmark_dt = calculate_landmark_dt(X,y)\n",
    "    #landmark_dnl = calculate_landmark_dnl(X,y)\n",
    "    #landmark_rnl = calculate_landmark_rnl(X,y)\n",
    "    #landmark_k1nn = calculate_landmark_k1nn(X,y)\n",
    "    \n",
    "    #pca=calculate_pca(X,y)\n",
    "    #landmark_PCA95PercentVariance = calculate_PCAFractionOfComponentsFor95PercentVariance(X,y,pca)\n",
    "    #landmark_PCAKurtosisFirstPC = calculate_PCAKurtosisFirstPC(X,y,pca)\n",
    "    #landmark_PCASkewnessFirstPC = calculate_PCASkewnessFirstPC(X,y,pca)\n",
    "    \n",
    "    \n",
    "    meta_features=np.array([file, nr_instances,log_nr_instances,nr_features,\\\n",
    "    log_nr_features,nr_classes,nr_numerical_features,nr_categorical_features,ratio_num_cat,\\\n",
    "    class_entropy, missing_val, ratio_missing_val,max_prob,min_prob, mean_prob,\\\n",
    "    std_dev,dataset_ratio,symbols_sum,symbols_mean,symbols_std_dev,\\\n",
    "    skew_min,skew_max,skew_mean,skew_std_dev,kurtosis_min,kurtosis_max,kurtosis_mean,kurtosis_std_dev])\n",
    "    #landmark_lda,landmark_nb,landmark_dt,landmark_dnl,landmark_rnl,landmark_k1nn,\n",
    "    #landmark_PCA95PercentVariance, landmark_PCAKurtosisFirstPC,landmark_PCASkewnessFirstPC])\n",
    "    \n",
    "    \n",
    "    return  meta_features\n",
    "\n",
    "def get_meta(file,data_type, target_col=None):\n",
    "    if(data_type==\"numpy\"):\n",
    "        dataset=np.load(file)\n",
    "        dataset=pd.DataFrame(dataset)\n",
    "    elif data_type ==\"csv\":\n",
    "        dataset = pd.read_csv(file, index_col=None, header=0)\n",
    "    return (np.array(extractMetaFeatures(dataset, file,classCol=target_col)[1:],dtype='float'))\n",
    "    #return extractMetaFeatures(dataset, file)[1:]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #file=\"uploads/blood.csv\"\n",
    "    file=\"uploads/digits_c.npy\"\n",
    "    dataset=np.load(\"uploads/digits_c.npy\")\n",
    "    dataset=pd.DataFrame(dataset)\n",
    "    #dataset = pd.read_csv(file, index_col=None, header=0)\n",
    "    print(np.array(extractMetaFeatures(dataset, file)[1:],dtype='float'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
